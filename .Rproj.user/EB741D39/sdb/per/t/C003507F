{
    "contents" : "---\ntitle: \"Practical Machine Learning Course Project\"\nauthor: \"WhitefishDontJump\"\ndate: \"June 2015\"\noutput:\n  html_document:\n    highlight: tango\n    keep_md: yes\n    theme: readable\n    toc: yes\n---\n\nThe source of data for this project is from\n\nVelloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. **Qualitative Activity Recognition of Weight Lifting Exercises.** Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013\n\nRead more: http://groupware.les.inf.puc-rio.br/har#ixzz3MCvfqKcP\n\n### Synopsis\n\n#### Exploration, Cleaning and Model Development\n\n1.  Response 'classe' is categorical (factor with 5 levels, A to E).\n2.  The predictors, after cleaning, are entirely integer or numeric, except user_name.\n3.  I will retain user_name as a factor variable in initial modelling.  It is possible that individual users uniquely condition the range of responses in other predictors.\n4.  I plan to use random forest (via caret's train function), and will use a three fold cross validation of rf results.\n5. There are 53 predictor variables after cleaning the raw dataset.  Given the size of the dataset, and selecting 50% of the training set, a 3 fold validation will yield more than 6000 observations per fold, on a base of more than 9800 observations in one half of the training dataset.\n\n#### Results Summary\n\n1. Initial model: Random Forest yielded a highly accurate predictive model (modela) with an estimated out of sample error rate of 1 %. It required 10 minutes to complete execution on my PC and correctly predicted the 20 submission test cases. The predictor user_name was not important: that is, the response, classe, was not very dependent on which user was being measured.\n\n2. One further Random Forest model: modelc (19 predictors based on varImp) removed user_name as well as many other predictors. modelc ran ~ 5 x faster than modela, with an estimated out of sample error comparable to modela. modelc also predicted the 20 submission test cases correctly.\n\n3. I conclude that modelc, with 19 predictors, provides a better balance of accuracy vs speed than modela.\n\n\n--------------------------------------------------------------------------\n\n### Initial Data Cleaning\n\n```{r GettingCleaningData, tidy=TRUE}\n\n    training <- read.csv(\"pml-training.csv\", \n                         na.strings = c(\"NA\", \"\", \"#DIV/0!\"))\n\n    testing <- read.csv(\"pml-testing.csv\", \n                        na.strings = c(\"NA\", \"\", \"#DIV/0!\"))\n\n##  View(training)\n\n### lots of missing data and/or NAs ##\n\n# I will be removing any columns with NAs as predictors in the training set.\n\n    cols2get <- colSums(is.na(training))==0\n    cleantraining <- training[, cols2get]\n\n###  leaves 60 columns\n## same cleaning for testing data (optional choice on my part)\n\n    cols2get2 <- colSums(is.na(testing))==0\n    cleantesting <- testing[,cols2get2]\n\n### remove row \"X\", as well as time date window columns\n###  which are not related to prediction of 'classe'\n\n      cleantraining <- cleantraining[,c(2,8:60)]\n      cleantesting <- cleantesting[,c(2,8:60)]\n      \n```\n\n### Initial Random Forest Model\n\n```{r Build Model, tidy=FALSE,message=FALSE}\n\n      require(parallel)  \n      require(doParallel)  ## parallel with 2 cores reduced runtime ~30%.\n      mc <- makeCluster(detectCores())\n      registerDoParallel(mc)\n\n      require(caret)\n      \n\n## create new partition in 'training set' and create training1, testing1 set.\n\n\n    set.seed(1506) ## for repeatability\n    trindex <- createDataPartition(cleantraining$classe, \n                                   p = 0.5, list = FALSE)\n\n    training1 <- cleantraining[trindex,]\n    testing1  <- cleantraining[-trindex,]\n\n##  Initial model with part of training set and 3 fold cross validation, \n##  using the random forest method and caret's train() function. \n##  Afterwards, test the model with other half of training set \n##  to validate and estimate out of sample accuracy.\n\n    controla <- trainControl(method=\"cv\", \n                             number = 3, \n                             allowParallel = TRUE)\n\n    modela <- train(x=training1[,-54],y=training1[,54], \n                    method=\"rf\",\n                    trControl = controla)\n\n\n```\n\n\n\n```{r model results and evaluation, tidy=TRUE}\n\n## determine in sample error, which vars are relevant and relative importance\n\n    modela\n\n    confusionMatrix(modela) ## in sample error\n\n    varImp(modela)\n\n## what about user_name? was user_name inportant?    \n    \n      head(varImp(modela)$importance,1)\n \n## user_name had minimal importance in the final model.\n\n## Out of Sample Error estimate\n\n##  I will estimate the out of sample error by applying the model to the second part of the training set (named testing1).\n\n    predicta  <- predict(modela, testing1) \n\n    confusionMatrix(predicta,testing1$classe)\n\n## model is very good, accuracy 0.99, and does not seem to be overfitting on the testing1 set. I am satisfied with the model result and also believe that using the entire training set can only increase the chance of overfitting.\n\n## Based on this favorable result, I use this model, 'modela', on the class testing set:\n\n    answers <- predict(modela, cleantesting)\n\n    answers\n\n## submitting 'answers' (via the function provided), scored 20 of 20 correct.\n\n```\n\n#### Initial Model comments:\n\nThe random forest method, tuned for 3 fold cross validation and a 50% training set size, created a prediction model with 99% accuracy in an out of sample test.\n\n\n----------------------------------------------------------------------------\n\n\n#### What about individual user conditioning the predictors?\n\nHere is a scatter plot of total_accel_arm and roll_belt, colored by user_name to examine the variation across users for one of the most important features in the model, roll_belt (see varImp output, above). Note: The selection of total_accel_arm for the plot was somewhat arbitrary; it isn't very important in the model but it is highly variable for each user, so the resulting plot displays the roll_belt values for better visualization of user differences. \n\n\n```{r Plots , fig.height=6, fig.width=8}\n\n    qplot(total_accel_arm, roll_belt, data=training1, color = user_name)\n\n```\n\nComments on the plot: Can this model be generalized to predict classe for other users not in the data set? While the plot confirms differences among users, it is not very important in the final model, as reported by varImp()$importance earlier in this report.\n\n\n\n----------------------------------------------------------------------------\n\n### Modelc: Reducing predictors to those with higher importance.\n\nNext, I will test an RF model with the set of predictors reduced to those with scaled importance >= 10, as reported by varImp() under modela. Because user_name had low importance in modela, it will be among those removed by this method.\n\n```{r reducedimensions,tidy=FALSE}\n\n\n##Use VarImp to remove lowest importance variables (scaled imp less than 10.0), leaving 19 predictors.\n\nimportance <- varImp(modela)$importance\nimportance$vars <- rownames(importance)\n\nimportance <- importance[order(importance$Overall, decreasing=TRUE),]\n\nimpCols <- importance[(importance$Overall >= 10.0),2]\nimpCols\n\ntraining2 <- training1[,c(impCols, \"classe\")]\ntesting2 <- testing1[,c(impCols, \"classe\")]\n\n### modelc will use reduced dimensionality\n\nmodelc <- train(x=training2[,-20],y=training2[,20], \n                    method=\"rf\", \n                    trControl = controla)\n\n      modelc      \n      confusionMatrix(modelc) ## in sample error\n\n##    modelc is comparable to modela\n\n      predictc  <- predict(modelc, testing2) \n\n      confusionMatrix(predictc,testing2$classe)\n\nanswersc <- predict(modelc, cleantesting)\n\n## compare modelc and modela predictions, if all match, sum=20\nsum(answersc == answers)\n\n```\n\nmodelc loses very little accuracy on test set (compared to modela) and achieves the same results on the course submission test set.  I conclude that the dimension reduction strategy was successful and reduced training time to about 20% of the time required for the original modela.\n\nThank you for reading and reviewing my work.\n\n--------------------------------------------------------------------------\n\nHere is sessionInfo(), for reference:\n\n```{r sessioninfo, tidy=TRUE}\n\nprint(sessionInfo(), locale=FALSE)\n\n```",
    "created" : 1427549448841.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2878704926",
    "id" : "C003507F",
    "lastKnownWriteTime" : 1434212946,
    "path" : "C:/Users/JR/Rdatacourse/github-repos/PracticalMachineLearning/PML-JUNE-2015NF-project-jhr.Rmd",
    "project_path" : "PML-JUNE-2015NF-project-jhr.Rmd",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 0,
    "source_on_save" : false,
    "type" : "r_markdown"
}